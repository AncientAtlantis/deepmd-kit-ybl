import paddle
from paddle_ops import prod_env_mat_a, prod_force_se_a, prod_force_se_a_grad2, prod_virial_se_a, prod_virial_se_a_grad2

import os,sys
import numpy as np
import unittest
import time

from deepmd.env import op_module

from deepmd.env import GLOBAL_TF_FLOAT_PRECISION
from deepmd.env import GLOBAL_NP_FLOAT_PRECISION
from deepmd.env import GLOBAL_ENER_FLOAT_PRECISION

import unittest

from deepmd.env import tf
from tensorflow.python.framework import ops

from common import Data

if GLOBAL_NP_FLOAT_PRECISION == np.float32 :
    global_default_fv_hh = 1e-2
    global_default_dw_hh = 1e-2
    global_default_places = 3
else :
    global_default_fv_hh = 1e-5
    global_default_dw_hh = 1e-4
    global_default_places = 5

class Inter():
    def setUp (self, 
               data, 
               pbc = True) :
        self.sess = tf.Session()
        self.data = data
        self.natoms = self.data.get_natoms()
        self.ntypes = self.data.get_ntypes()
        self.sel_a = [12,24]
        self.sel_r = [0,0]
        self.rcut_a = -1
        self.rcut_r_smth = 2.45
        self.rcut_r = 10.0
        self.nnei_a = np.cumsum(self.sel_a)[-1]
        self.nnei_r = np.cumsum(self.sel_r)[-1]
        self.nnei = self.nnei_a + self.nnei_r
        self.ndescrpt_a = self.nnei_a * 4
        self.ndescrpt_r = self.nnei_r * 1
        self.ndescrpt = self.ndescrpt_a + self.ndescrpt_r
        davg = np.zeros ([self.ntypes, self.ndescrpt])
        dstd = np.ones  ([self.ntypes, self.ndescrpt])
        self.t_avg = tf.constant(davg.astype(GLOBAL_NP_FLOAT_PRECISION))
        self.t_std = tf.constant(dstd.astype(GLOBAL_NP_FLOAT_PRECISION))
        if pbc:
            self.default_mesh = np.zeros (6, dtype = np.int32)
            self.default_mesh[3] = 2
            self.default_mesh[4] = 2
            self.default_mesh[5] = 2
        else:
            self.default_mesh = np.array([], dtype = np.int32)
        # make place holder
        self.coord      = tf.placeholder(GLOBAL_TF_FLOAT_PRECISION, [None, self.natoms[0] * 3], name='t_coord')
        self.box        = tf.placeholder(GLOBAL_TF_FLOAT_PRECISION, [None, 9], name='t_box')
        self.type       = tf.placeholder(tf.int32,   [None, self.natoms[0]], name = "t_type")
        self.tnatoms    = tf.placeholder(tf.int32,   [None], name = "t_natoms")
        self.efield     = tf.placeholder(GLOBAL_TF_FLOAT_PRECISION, [None, self.natoms[0] * 3], name='t_efield')
        
    def _net (self,
             inputs, 
             name,
              reuse = False) :
        with tf.variable_scope(name, reuse=reuse):
            net_w = tf.get_variable ('net_w', 
                                     [self.ndescrpt], 
                                     GLOBAL_TF_FLOAT_PRECISION,
                                     tf.constant_initializer (self.net_w_i))
        dot_v = tf.matmul (tf.reshape (inputs, [-1, self.ndescrpt]),
                           tf.reshape (net_w, [self.ndescrpt, 1]))
        return tf.reshape (dot_v, [-1])
        
    def comp_ef_before (self, 
                 dcoord, 
                 dbox, 
                 dtype,
                 tnatoms,
                 name,
                 reuse = None) :
        descrpt, descrpt_deriv, rij, nlist \
            = op_module.prod_env_mat_a (dcoord, 
                                       dtype,
                                       tnatoms,
                                       dbox, 
                                       tf.constant(self.default_mesh),
                                       self.t_avg,
                                       self.t_std,
                                       rcut_a = self.rcut_a, 
                                       rcut_r = self.rcut_r, 
                                       rcut_r_smth = self.rcut_r_smth,
                                       sel_a = self.sel_a, 
                                       sel_r = self.sel_r)
        inputs_reshape = tf.reshape (descrpt, [-1, self.ndescrpt])
        atom_ener = self._net (inputs_reshape, name, reuse = reuse)
        atom_ener_reshape = tf.reshape(atom_ener, [-1, self.natoms[0]])        
        energy = tf.reduce_sum (atom_ener_reshape, axis = 1)        
        net_deriv_ = tf.gradients (atom_ener, inputs_reshape)
        net_deriv = net_deriv_[0]
        net_deriv_reshape = tf.reshape (net_deriv, [-1, self.natoms[0] * self.ndescrpt]) 

        return net_deriv_reshape, descrpt_deriv, nlist, tnatoms, rij, energy


    def comp_f_dw (self, 
                   dcoord, 
                   dbox, 
                   dtype,                 
                   tnatoms,
                   name,
                   reuse = None) :
        net_deriv_reshape, descrpt_deriv, nlist, tnatoms, rij, energy = self.comp_ef_before(dcoord, dbox, dtype, tnatoms, name, reuse)
        
        return net_deriv_reshape, descrpt_deriv, nlist, tnatoms, rij, energy


def force_test (inter, 
                testCase, 
                places = global_default_places, 
                hh = global_default_fv_hh, 
                suffix = '') :
    # set weights
    w0 = np.ones (inter.ndescrpt)
    inter.net_w_i = np.copy(w0)
    # make network
    net_deriv_reshape, descrpt_deriv, nlist, tnatoms, rij, energy \
        = inter.comp_ef_before (inter.coord, inter.box, inter.type, inter.tnatoms, name = "test_f" + suffix)
    inter.sess.run (tf.global_variables_initializer())
    # get data
    dcoord, dbox, dtype = inter.data.get_data ()
    defield = inter.data.efield
    # cmp e0, f0
    [dnet_deriv_reshape, ddescrpt_deriv, dnlist, dtnatoms, drij, denergy] = \
        inter.sess.run ([net_deriv_reshape, descrpt_deriv, nlist, tnatoms, rij, energy], 
                                        feed_dict = {
                                            inter.coord:     dcoord,
                                            inter.box:       dbox,
                                            inter.type:      dtype,
                                            inter.efield:    defield,
                                            inter.tnatoms:   inter.natoms}
    )
    force = prod_force_se_a (paddle.to_tensor(dnet_deriv_reshape.reshape(dnet_deriv_reshape.shape), dtype="float32"), 
                                        paddle.to_tensor(ddescrpt_deriv.reshape(ddescrpt_deriv.shape), dtype="float32"), 
                                        paddle.to_tensor(dnlist.reshape(dnlist.shape), dtype="int32"), 
                                        paddle.to_tensor (dtnatoms.reshape(dtnatoms.shape), dtype="int32"),
                                        n_a_sel = inter.nnei_a, 
                                        n_r_sel = inter.nnei_r)
    dforce = force.numpy()
    # dim force
    sel_idx = np.arange(inter.natoms[0])    
    for idx in sel_idx:
        for dd in range(3):
            dcoordp = np.copy(dcoord)
            dcoordm = np.copy(dcoord)
            dcoordp[0,idx*3+dd] = dcoord[0,idx*3+dd] + hh
            dcoordm[0,idx*3+dd] = dcoord[0,idx*3+dd] - hh
            [enerp] = inter.sess.run ([energy], 
                                     feed_dict = {
                                         inter.coord:     dcoordp,
                                         inter.box:       dbox,
                                         inter.type:      dtype,
                                         inter.efield:    defield,
                                         inter.tnatoms:   inter.natoms}
            )
            [enerm] = inter.sess.run ([energy], 
                                     feed_dict = {
                                         inter.coord:     dcoordm,
                                         inter.box:       dbox,
                                         inter.type:      dtype,
                                         inter.efield:    defield,
                                         inter.tnatoms:   inter.natoms}
            )
            c_force = -(enerp[0] - enerm[0]) / (2*hh)
            testCase.assertAlmostEqual(c_force, dforce[0,idx*3+dd], 
                                       places = places,
                                       msg = "force component [%d,%d] failed" % (idx, dd))

def force_dw_test (inter, 
                   testCase,
                   places = global_default_places,
                   hh = global_default_dw_hh, 
                   suffix = '') :
    paddle.set_device("cpu")
    grad = [10, 9.9, 9.8, 9.7, 9.6, 9.5, 9.4, 9.3, 9.2, 9.1, 9, 8.9, 8.8, 8.7, 8.6, 8.5, 8.4, 8.3]
    env_deriv =[0.132277, 0.0164878, -0.0138647, 0.129675, 0.0204174, -0.0171692, 0.0204174, -0.0315835, -0.00214007, -0.0171692, -0.00214007, -0.0323289, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.794652, 0.332895, 0.601358, 0.154122, -0.502001, -0.906841, -0.502001, -0.833906, 0.379893, -0.906841, 0.379893, -0.357946, 0.420626, 0.761133, -0.500746, -0.644254, 0.635525, -0.418109, 0.635525, 0.154532, -0.756578, -0.418109, -0.756578, -0.497717, 0.122407, -0.00166313, 0.0139703, 0.121234, -0.00203467, 0.0170912, -0.00203467, -0.02849, -0.000232218, 0.0170912, -0.000232218, -0.0265671, 0.0579457, 0.00861355, -0.00809152, 0.0565034, 0.00941713, -0.00884639, 0.00941713, -0.00544832, -0.001315, -0.00884639, -0.001315, -0.00561285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.794652, -0.332895, -0.601358, 0.154122, -0.502001, -0.906841, -0.502001, -0.833906, 0.379893, -0.906841, 0.379893, -0.357946, 0.0688432, 0.00209593, -0.014994, 0.0668002, 0.00232169, -0.016609, 0.00232169, -0.0093878, -0.000505661, -0.016609, -0.000505661, -0.00584106, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.302593, 0.117385, -0.276507, 0.0349136, 0.154094, -0.362978, 0.154094, -0.302529, -0.14081, -0.362978, -0.14081, -0.0306208, 0.0655508, -0.00533898, -0.00207627, 0.0652388, -0.00599163, -0.00233008, -0.00599163, -0.00783703, 0.00018978, -0.00233008, 0.00018978, -0.00825124, 0.014092, 0.000952162, -0.00321015, 0.0136766, 0.000966739, -0.00325929, 0.000966739, -0.000565869, -0.000220223, -0.00325929, -0.000220223, 0.000111275, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.420626, -0.761133, 0.500746, -0.644254, 0.635525, -0.418109, 0.635525, 0.154532, -0.756578, -0.418109, -0.756578, -0.497717, 0.172652, -0.0177648, 0.00721696, 0.170854, -0.0238531, 0.00969033, -0.0238531, -0.0585143, -0.000997076, 0.00969033, -0.000997076, -0.0605636, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.302593, -0.117385, 0.276507, 0.0349136, 0.154094, -0.362978, 0.154094, -0.302529, -0.14081, -0.362978, -0.14081, -0.0306208, 0.132989, -0.0330433, 0.0375306, 0.119679, -0.0393667, 0.0447128, -0.0393667, -0.028978, -0.0111096, 0.0447128, -0.0111096, -0.0261409, 0.0970921, -0.00241523, -0.00289827, 0.0969925, -0.00284894, -0.00341873, -0.00284894, -0.0174641, 8.50431e-05, -0.00341873, 8.50431e-05, -0.0174329, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.132277, -0.0164878, 0.0138647, 0.129675, 0.0204174, -0.0171692, 0.0204174, -0.0315835, -0.00214007, -0.0171692, -0.00214007, -0.0323289, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1803, -0.58898, 0.94958, -1.07023, -0.187287, 0.301952, -0.187287, -0.515755, -0.986378, 0.301952, -0.986378, 0.462724, 1.0053, 0.24304, -0.276182, 0.818336, 0.455219, -0.517294, 0.455219, -0.954562, -0.12506, -0.517294, -0.12506, -0.922501, -0.172652, 0.0177648, -0.00721696, 0.170854, -0.0238531, 0.00969033, -0.0238531, -0.0585143, -0.000997076, 0.00969033, -0.000997076, -0.0605636, -0.0688432, -0.00209593, 0.014994, 0.0668002, 0.00232169, -0.016609, 0.00232169, -0.0093878, -0.000505661, -0.016609, -0.000505661, -0.00584106, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.1803, 0.58898, -0.94958, -1.07023, -0.187287, 0.301952, -0.187287, -0.515755, -0.986378, 0.301952, -0.986378, 0.462724, -0.122407, 0.00166313, -0.0139703, 0.121234, -0.00203467, 0.0170912, -0.00203467, -0.02849, -0.000232218, 0.0170912, -0.000232218, -0.0265671, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.260259, 0.243137, -0.356144, -0.198414, 0.238915, -0.34996, 0.238915, -0.230957, -0.326936, -0.34996, -0.326936, 0.0247386, -0.132989, 0.0330433, -0.0375306, 0.119679, -0.0393667, 0.0447128, -0.0393667, -0.028978, -0.0111096, 0.0447128, -0.0111096, -0.0261409, -0.0655508, 0.00533898, 0.00207627, 0.0652388, -0.00599163, -0.00233008, -0.00599163, -0.00783703, 0.00018978, -0.00233008, 0.00018978, -0.00825124, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1.0053, -0.24304, 0.276182, 0.818336, 0.455219, -0.517294, 0.455219, -0.954562, -0.12506, -0.517294, -0.12506, -0.922501, -0.0579457, -0.00861355, 0.00809152, 0.0565034, 0.00941713, -0.00884639, 0.00941713, -0.00544832, -0.001315, -0.00884639, -0.001315, -0.00561285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.260259, -0.243137, 0.356144, -0.198414, 0.238915, -0.34996, 0.238915, -0.230957, -0.326936, -0.34996, -0.326936, 0.0247386, -0.0970921, 0.00241523, 0.00289827, 0.0969925, -0.00284894, -0.00341873, -0.00284894, -0.0174641, 8.50431e-05, -0.00341873, 8.50431e-05, -0.0174329, -0.014092, -0.000952162, 0.00321015, 0.0136766, 0.000966739, -0.00325929, 0.000966739, -0.000565869, -0.000220223, -0.00325929, -0.000220223, 0.000111275, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,]
    nlist = [33, -1, -1, -1, -1, 1, 32, 34, 35, -1, 0, 33, -1, -1, -1, 32, 34, 35, -1, -1, 6, 3, -1, -1, -1, 7, 4, 5, -1, -1, 6, -1, -1, -1, -1, 4, 5, 2, 7, -1, 3, 6, -1, -1, -1, 5, 2, 7, -1, -1, 3, 6, -1, -1, -1, 4, 2, 7, -1, -1]
    natoms = [6,6,6,6,6,6]
    dgrad = np.array(grad, dtype="float32").reshape((1,18))
    denv_deriv = np.array(env_deriv, dtype="float32").reshape((1, 720))
    dnlist = np.array(nlist, dtype="int32").reshape((1, 60))
    dnatoms = np.array(natoms, dtype="int32").reshape((6))
    grad_net = prod_force_se_a_grad2 (paddle.to_tensor(dgrad, dtype="float32"), 
                                    paddle.to_tensor(np.ones(shape =(1, 240), dtype="float32"), dtype="float32"), 
                                    paddle.to_tensor(denv_deriv, dtype="float32"), 
                                    paddle.to_tensor(dnlist, dtype="int32"), 
                                    paddle.to_tensor (dnatoms, dtype="int32"),
                                    n_a_sel = 5, 
                                    n_r_sel = 5)
    expected_grad_net = [5.01828,  4.97546, -0.09569, -1.15305,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000, -0.61704,  1.06623,  0.15319,  0.24608,  5.28467, -2.59553,  3.00729, -8.19962,  5.03021,  5.02151, -0.86956,  0.26289,  2.75500,  2.70125,  0.22900, -0.54729,  0.00000,  0.00000,  0.00000,  0.00000, -0.61704, -1.06623, -0.15319, -0.24608,  2.32844,  2.23467, -0.16758, -0.70940,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  1.74748, -0.30379, -1.11004, -3.49833,  2.42774,  2.39284, -0.45567, -0.22216,  0.60993,  0.59054,  0.02135, -0.15332,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  5.28467,  2.59553, -3.00729,  8.19962,  4.77234,  4.62396, -1.90919, -0.44792,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  1.74748,  0.30379,  1.11004,  3.49833,  4.06655,  3.57849, -2.07817,  0.88468,  3.61241,  3.58881, -0.57839, -0.39969,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  5.01828, -4.97546,  0.09569,  1.15305,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.07573, -3.82089, -2.40143, -0.67375,  9.64382,  8.39638, -2.48922, -9.00792,  4.77234, -4.62396,  1.90919,  0.44792,  2.32844, -2.23467,  0.16758,  0.70940,  0.00000,  0.00000,  0.00000,  0.00000,  0.07573,  3.82089,  2.40143,  0.67375,  5.03021, -5.02151,  0.86956, -0.26289,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  1.44012, -1.15994, -0.66718, -3.33981,  4.06655, -3.57849,  2.07817, -0.88468,  2.42774, -2.39284,  0.45567,  0.22216,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  9.64382, -8.39638,  2.48922,  9.00792,  2.75500, -2.70125, -0.22900,  0.54729,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  1.44012,  1.15994,  0.66718,  3.33981,  3.61241, -3.58881,  0.57839,  0.39969,  0.60993, -0.59054, -0.02135,  0.15332,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000]
    dexpected_grad_net = np.array(expected_grad_net, dtype="float32").reshape((1, 240))
    for i in range(240):
        np.array_equal(grad_net.numpy(), dexpected_grad_net)


def virial_test (inter, 
                 testCase, 
                 places = global_default_places, 
                 hh = global_default_fv_hh, 
                 suffix = '') :
    # set weights
    w0 = np.ones (inter.ndescrpt)
    inter.net_w_i = np.copy(w0)
    # make network
    net_deriv_reshape, descrpt_deriv, nlist, tnatoms, rij, energy \
        = inter.comp_ef_before (inter.coord, inter.box, inter.type, inter.tnatoms, name = "test_v" + suffix)
    inter.sess.run (tf.global_variables_initializer())
    # get data
    dcoord, dbox, dtype, defield = inter.data.get_test_box_data(hh)
    # cmp e, f, v
    [dnet_deriv_reshape, ddescrpt_deriv, dnlist, dtnatoms, drij, denergy] = \
        inter.sess.run ([net_deriv_reshape, descrpt_deriv, nlist, tnatoms, rij, energy], 
                                        feed_dict = {
                                            inter.coord:     dcoord,
                                            inter.box:       dbox,
                                            inter.type:      dtype,
                                            inter.efield:    defield,
                                            inter.tnatoms:   inter.natoms}
    )
    paddle.set_device("cpu")
    virial, atom_vir = prod_virial_se_a (paddle.to_tensor(dnet_deriv_reshape.reshape(dnet_deriv_reshape.shape), dtype="float32"), 
                                        paddle.to_tensor(ddescrpt_deriv.reshape(ddescrpt_deriv.shape), dtype="float32"), 
                                        paddle.to_tensor(drij.reshape(drij.shape), dtype="float32"),
                                        paddle.to_tensor(dnlist.reshape(dnlist.shape), dtype="int32"), 
                                        paddle.to_tensor (dtnatoms.reshape(dtnatoms.shape), dtype="int32"),
                                        n_a_sel = inter.nnei_a, 
                                        n_r_sel = inter.nnei_r)
    dvirial = virial.numpy()
    dvirial.shape
    # print("dvirial shape is {} \n value is : {}".format(dvirial.shape, dvirial))                                  
    ana_vir = dvirial[0].reshape([3,3])
    num_vir = np.zeros([3,3])
    for ii in range(3):
        for jj in range(3):
            ep = denergy[1+(ii*3+jj)*2+0]
            em = denergy[1+(ii*3+jj)*2+1]
            num_vir[ii][jj] = -(ep - em) / (2.*hh)
    num_vir = np.transpose(num_vir, [1,0])    
    box3 = dbox[0].reshape([3,3])
    num_vir = np.matmul(num_vir, box3)
    for ii in range(3):
        for jj in range(3):
            testCase.assertAlmostEqual(ana_vir[ii][jj], num_vir[ii][jj],
                                       places=places, 
                                       msg = 'virial component %d %d ' % (ii,jj))
    
def virial_dw_test (inter, 
                   testCase,
                   places = global_default_places,
                   hh = global_default_dw_hh, 
                   suffix = '') :
    paddle.set_device("cpu")
    grad = [10, 9, 8, 7, 6, 5, 4, 3, 2]
    env_deriv =[0.132277, 0.0164878, -0.0138647, 0.129675, 0.0204174, -0.0171692, 0.0204174, -0.0315835, -0.00214007, -0.0171692, -0.00214007, -0.0323289, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.794652, 0.332895, 0.601358, 0.154122, -0.502001, -0.906841, -0.502001, -0.833906, 0.379893, -0.906841, 0.379893, -0.357946, 0.420626, 0.761133, -0.500746, -0.644254, 0.635525, -0.418109, 0.635525, 0.154532, -0.756578, -0.418109, -0.756578, -0.497717, 0.122407, -0.00166313, 0.0139703, 0.121234, -0.00203467, 0.0170912, -0.00203467, -0.02849, -0.000232218, 0.0170912, -0.000232218, -0.0265671, 0.0579457, 0.00861355, -0.00809152, 0.0565034, 0.00941713, -0.00884639, 0.00941713, -0.00544832, -0.001315, -0.00884639, -0.001315, -0.00561285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.794652, -0.332895, -0.601358, 0.154122, -0.502001, -0.906841, -0.502001, -0.833906, 0.379893, -0.906841, 0.379893, -0.357946, 0.0688432, 0.00209593, -0.014994, 0.0668002, 0.00232169, -0.016609, 0.00232169, -0.0093878, -0.000505661, -0.016609, -0.000505661, -0.00584106, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.302593, 0.117385, -0.276507, 0.0349136, 0.154094, -0.362978, 0.154094, -0.302529, -0.14081, -0.362978, -0.14081, -0.0306208, 0.0655508, -0.00533898, -0.00207627, 0.0652388, -0.00599163, -0.00233008, -0.00599163, -0.00783703, 0.00018978, -0.00233008, 0.00018978, -0.00825124, 0.014092, 0.000952162, -0.00321015, 0.0136766, 0.000966739, -0.00325929, 0.000966739, -0.000565869, -0.000220223, -0.00325929, -0.000220223, 0.000111275, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.420626, -0.761133, 0.500746, -0.644254, 0.635525, -0.418109, 0.635525, 0.154532, -0.756578, -0.418109, -0.756578, -0.497717, 0.172652, -0.0177648, 0.00721696, 0.170854, -0.0238531, 0.00969033, -0.0238531, -0.0585143, -0.000997076, 0.00969033, -0.000997076, -0.0605636, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.302593, -0.117385, 0.276507, 0.0349136, 0.154094, -0.362978, 0.154094, -0.302529, -0.14081, -0.362978, -0.14081, -0.0306208, 0.132989, -0.0330433, 0.0375306, 0.119679, -0.0393667, 0.0447128, -0.0393667, -0.028978, -0.0111096, 0.0447128, -0.0111096, -0.0261409, 0.0970921, -0.00241523, -0.00289827, 0.0969925, -0.00284894, -0.00341873, -0.00284894, -0.0174641, 8.50431e-05, -0.00341873, 8.50431e-05, -0.0174329, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.132277, -0.0164878, 0.0138647, 0.129675, 0.0204174, -0.0171692, 0.0204174, -0.0315835, -0.00214007, -0.0171692, -0.00214007, -0.0323289, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1803, -0.58898, 0.94958, -1.07023, -0.187287, 0.301952, -0.187287, -0.515755, -0.986378, 0.301952, -0.986378, 0.462724, 1.0053, 0.24304, -0.276182, 0.818336, 0.455219, -0.517294, 0.455219, -0.954562, -0.12506, -0.517294, -0.12506, -0.922501, -0.172652, 0.0177648, -0.00721696, 0.170854, -0.0238531, 0.00969033, -0.0238531, -0.0585143, -0.000997076, 0.00969033, -0.000997076, -0.0605636, -0.0688432, -0.00209593, 0.014994, 0.0668002, 0.00232169, -0.016609, 0.00232169, -0.0093878, -0.000505661, -0.016609, -0.000505661, -0.00584106, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.1803, 0.58898, -0.94958, -1.07023, -0.187287, 0.301952, -0.187287, -0.515755, -0.986378, 0.301952, -0.986378, 0.462724, -0.122407, 0.00166313, -0.0139703, 0.121234, -0.00203467, 0.0170912, -0.00203467, -0.02849, -0.000232218, 0.0170912, -0.000232218, -0.0265671, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.260259, 0.243137, -0.356144, -0.198414, 0.238915, -0.34996, 0.238915, -0.230957, -0.326936, -0.34996, -0.326936, 0.0247386, -0.132989, 0.0330433, -0.0375306, 0.119679, -0.0393667, 0.0447128, -0.0393667, -0.028978, -0.0111096, 0.0447128, -0.0111096, -0.0261409, -0.0655508, 0.00533898, 0.00207627, 0.0652388, -0.00599163, -0.00233008, -0.00599163, -0.00783703, 0.00018978, -0.00233008, 0.00018978, -0.00825124, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1.0053, -0.24304, 0.276182, 0.818336, 0.455219, -0.517294, 0.455219, -0.954562, -0.12506, -0.517294, -0.12506, -0.922501, -0.0579457, -0.00861355, 0.00809152, 0.0565034, 0.00941713, -0.00884639, 0.00941713, -0.00544832, -0.001315, -0.00884639, -0.001315, -0.00561285, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.260259, -0.243137, 0.356144, -0.198414, 0.238915, -0.34996, 0.238915, -0.230957, -0.326936, -0.34996, -0.326936, 0.0247386, -0.0970921, 0.00241523, 0.00289827, 0.0969925, -0.00284894, -0.00341873, -0.00284894, -0.0174641, 8.50431e-05, -0.00341873, 8.50431e-05, -0.0174329, -0.014092, -0.000952162, 0.00321015, 0.0136766, 0.000966739, -0.00325929, 0.000966739, -0.000565869, -0.000220223, -0.00325929, -0.000220223, 0.000111275, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]    
    rij = [3.53, 0.44, -0.37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.74, 0.31, 0.56, 0.42, 0.76, -0.5, 3.68, -0.05, 0.42, 4.44, 0.66, -0.62, 0, 0, 0, 0.74, -0.31, -0.56, 4.27, 0.13, -0.93, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.16, 0.45, -1.06, 4.42, -0.36, -0.14, 5.18, 0.35, -1.18, 0, 0, 0, 0, 0, 0, -0.42, -0.76, 0.5, 3.11, -0.32, 0.13, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1.16, -0.45, 1.06, 3.26, -0.81, 0.92, 4.02, -0.1, -0.12, 0, 0, 0, 0, 0, 0, -3.53, -0.44, 0.37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.15, -0.49, 0.79, 0.91, 0.22, -0.25, -3.11, 0.32, -0.13, -4.27, -0.13, 0.93, 0, 0, 0, -0.15, 0.49, -0.79, -3.68, 0.05, -0.42, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.76, 0.71, -1.04, -3.26, 0.81, -0.92, -4.42, 0.36, 0.14, 0, 0, 0, 0, 0, 0, -0.91, -0.22, 0.25, -4.44, -0.66, 0.62, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.76, -0.71, 1.04, -4.02, 0.1, 0.12, -5.18, -0.35, 1.18, 0, 0, 0, 0, 0, 0]
    nlist = [33, -1, -1, -1, -1, 1, 32, 34, 35, -1, 0, 33, -1, -1, -1, 32, 34, 35, -1, -1, 6, 3, -1, -1, -1, 7, 4, 5, -1, -1, 6, -1, -1, -1, -1, 4, 5, 2, 7, -1, 3, 6, -1, -1, -1, 5, 2, 7, -1, -1, 3, 6, -1, -1, -1, 4, 2, 7, -1, -1]
    natoms = [6,6,6,6,6,6]
    dgrad = np.array(grad, dtype="float32").reshape((1,9))
    denv_deriv = np.array(env_deriv, dtype="float32").reshape((1, 720))
    drij = np.array(rij, dtype="float32").reshape((1, 180))
    dnlist = np.array(nlist, dtype="int32").reshape((1, 60))
    dnatoms = np.array(natoms, dtype="int32").reshape((6))
    grad_net = prod_virial_se_a_grad2 (paddle.to_tensor(dgrad, dtype="float32"), 
                                    paddle.to_tensor(np.ones(shape =(1, 240), dtype="float32"), dtype="float32"), 
                                    paddle.to_tensor(denv_deriv, dtype="float32"), 
                                    paddle.to_tensor(drij, dtype="float32"),
                                    paddle.to_tensor(dnlist, dtype="int32"), 
                                    paddle.to_tensor (dnatoms, dtype="int32"),
                                    n_a_sel = 5, 
                                    n_r_sel = 5)
    expected_grad_net = [5.01828,  4.97546, -0.09569, -1.15305,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000, -0.61704,  1.06623,  0.15319,  0.24608,  5.28467, -2.59553,  3.00729, -8.19962,  5.03021,  5.02151, -0.86956,  0.26289,  2.75500,  2.70125,  0.22900, -0.54729,  0.00000,  0.00000,  0.00000,  0.00000, -0.61704, -1.06623, -0.15319, -0.24608,  2.32844,  2.23467, -0.16758, -0.70940,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  1.74748, -0.30379, -1.11004, -3.49833,  2.42774,  2.39284, -0.45567, -0.22216,  0.60993,  0.59054,  0.02135, -0.15332,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  5.28467,  2.59553, -3.00729,  8.19962,  4.77234,  4.62396, -1.90919, -0.44792,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  1.74748,  0.30379,  1.11004,  3.49833,  4.06655,  3.57849, -2.07817,  0.88468,  3.61241,  3.58881, -0.57839, -0.39969,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  5.01828, -4.97546,  0.09569,  1.15305,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.07573, -3.82089, -2.40143, -0.67375,  9.64382,  8.39638, -2.48922, -9.00792,  4.77234, -4.62396,  1.90919,  0.44792,  2.32844, -2.23467,  0.16758,  0.70940,  0.00000,  0.00000,  0.00000,  0.00000,  0.07573,  3.82089,  2.40143,  0.67375,  5.03021, -5.02151,  0.86956, -0.26289,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  1.44012, -1.15994, -0.66718, -3.33981,  4.06655, -3.57849,  2.07817, -0.88468,  2.42774, -2.39284,  0.45567,  0.22216,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  9.64382, -8.39638,  2.48922,  9.00792,  2.75500, -2.70125, -0.22900,  0.54729,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  1.44012,  1.15994,  0.66718,  3.33981,  3.61241, -3.58881,  0.57839,  0.39969,  0.60993, -0.59054, -0.02135,  0.15332,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000,  0.00000]
    dexpected_grad_net = np.array(expected_grad_net, dtype="float32").reshape((1, 240))
    for i in range(240):
        np.array_equal(grad_net.numpy(), dexpected_grad_net)

class TestPdSmooth(Inter, unittest.TestCase):
    def setUp(self):
        self.places = 5
        data = Data()
        Inter.setUp(self, data)

    def test_force (self) :
        force_test(self, self, suffix = '_smth')
    
    def test_force_dw (self) :
        force_dw_test(self, self, suffix = '_smth')

    def test_virial (self) :
        virial_test(self, self, suffix = '_smth')

    def test_virial_dw (self) :
        virial_dw_test(self, self, suffix = '_smth')
if __name__ == '__main__':
    unittest.main()
